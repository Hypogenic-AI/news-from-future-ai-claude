# Cloned Repositories

This directory contains code repositories relevant to the "News from the Future" research.

## Repository 1: ForecastBench

- **URL**: https://github.com/forecastingresearch/forecastbench
- **Purpose**: Dynamic benchmark for LLM forecasting evaluation
- **Location**: `code/forecastbench/`
- **License**: See repository

### Key Files
- `src/` - Core benchmark code
- `src/nightly_update_workflow/` - Orchestrates nightly updates
- Question generation and evaluation code

### How It Relates to Research
- Provides methodology for sourcing forecasting questions from prediction markets
- Evaluation framework for forecasting accuracy (Brier scores)
- Question bank with Metaculus, Polymarket, Manifold questions
- Human comparison baselines (superforecasters, crowd)

### Usage
```python
# See src/ for question generation and evaluation
# API documentation at https://www.forecastbench.org
```

---

## Repository 2: LLM Forecasting

- **URL**: https://github.com/dannyallover/llm_forecasting
- **Purpose**: LLM forecasting experiments and methodology
- **Location**: `code/llm_forecasting/`

### Key Features
- Prompting strategies for forecasting
- Retrieval-augmented generation (RAG) for current events
- Evaluation scripts

### How It Relates to Research
- Provides tested prompting approaches
- RAG pipeline for incorporating current context
- Baseline implementation patterns

---

## Repository 3: LLM Misinformation

- **URL**: https://github.com/llm-misinformation/llm-misinformation
- **Purpose**: LLM-generated misinformation detection (ICLR 2024)
- **Location**: `code/llm-misinformation/`

### Key Features
- LLMFake dataset: misinformation generated by 7 LLMs
- 7 generation methods
- Detection baselines and evaluation

### How It Relates to Research
- Generation methods for realistic content
- Quality evaluation approaches
- Understanding what makes LLM content detectable

### Key Files
- Dataset generation scripts
- Detection model code
- Evaluation metrics

---

## Usage Notes

### Installing Dependencies
Each repository has its own requirements. Create separate environments if needed:

```bash
cd code/forecastbench
pip install -r requirements.txt
```

### Adapting for "News from the Future"

1. **From ForecastBench**: Adapt question sourcing and probability retrieval
2. **From LLM Forecasting**: Reuse prompting templates and RAG pipeline
3. **From LLM Misinformation**: Adapt generation methods for news articles

### Recommended Integration

```python
# Pseudocode for combining approaches

# 1. Get forecasting question and probability from prediction market
question, probability = get_from_forecastbench_api(topic)

# 2. Use RAG to retrieve current context
context = retrieve_news_context(question)  # from llm_forecasting

# 3. Generate news article conditioned on probability
prompt = create_probability_conditioned_prompt(
    question, probability, context
)
news_article = generate_with_llm(prompt)

# 4. Evaluate plausibility
score = evaluate_plausibility(news_article)  # from llm-misinformation
```

---

## Additional Repositories to Consider

- **ForecastBench Datasets**: https://github.com/forecastingresearch/forecastbench-datasets
- **FakeNewsNet**: https://github.com/KaiDMML/FakeNewsNet
- **ICTMCG LLM Misinformation Research**: https://github.com/ICTMCG/LLM-for-misinformation-research (paper list)
